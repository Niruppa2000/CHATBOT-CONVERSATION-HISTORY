{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8477a9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4abb299",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f960e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"Gemma2-9b-It\", groq_api_key=groq_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2dbfc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Niruppa! It's great to meet you.\\n\\nI'm happy to hear you're a GenAI enthusiast! What aspects of generative AI are you most interested in? \\n\\nDo you have any specific projects you're working on or questions you'd like to discuss? I'm always eager to learn more about how people are using and exploring the potential of this technology.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 23, 'total_tokens': 107, 'completion_time': 0.152727273, 'prompt_time': 0.00132835, 'queue_time': 0.25663950900000004, 'total_time': 0.154055623}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--88016287-02e4-4a1f-b851-e9850ed87062-0', usage_metadata={'input_tokens': 23, 'output_tokens': 84, 'total_tokens': 107})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "model.invoke([HumanMessage(content=\"Hi,My name is Niruppa and I am a Genai enthusiast\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b3b3a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and don't have access to any personal information about you, including your name or your likes. \\n\\nYou introduced yourself as Niruppa earlier in our conversation!  \\n\\nWhat are some things you enjoy? I'd love to learn more about you. ðŸ˜Š  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 107, 'total_tokens': 176, 'completion_time': 0.125454545, 'prompt_time': 0.00351189, 'queue_time': 0.25373704999999996, 'total_time': 0.128966435}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--96106b3c-e49b-4d49-883a-0162b655e96a-0', usage_metadata={'input_tokens': 107, 'output_tokens': 69, 'total_tokens': 176})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi,My name is Niruppa and I am a Genai enthusiast\"),\n",
    "        AIMessage(content=\"Hi Niruppa,\\n\\nIt's nice to meet you! I'm glad to hear you're a GenAI enthusiast. \\n\\nWhat aspects of generative AI are you most interested in? I'm always happy to chat about the latest developments, applications, or even ethical considerations surrounding this exciting field.\\n\"),\n",
    "        HumanMessage(content=\"Hey whats my name and what do I like?\")\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8fa312",
   "metadata": {},
   "source": [
    "Message history is essential for maintaining context across different sessions and users. In real-world applications, multiple sessions may occur simultaneously, and each session must retain its own context. This is managed using session IDs and message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28dca97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message history\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d976dfb",
   "metadata": {},
   "source": [
    "Implementing Session-Based Message History\n",
    "We create a function to manage session-based message history. Each session ID corresponds to a separate chat history, stored in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb9b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c4b32",
   "metadata": {},
   "source": [
    "Using RunnableWithMessageHistory for Stateful Chains\n",
    "We wrap the model with RunnableWithMessageHistory, providing the model and the session history function. This allows the chatbot to maintain state across interactions for each session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea5b43b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8dd65f",
   "metadata": {},
   "source": [
    "Configuring and Using Session IDs\n",
    "We define a configuration with a session ID and use it to interact with the chatbot. The session ID ensures that the chatbot remembers the context for that particular session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33130eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Niruppa, it's nice to meet you! \n",
      "\n",
      "I'm glad to hear you're a GenAI enthusiast. It's an exciting field with a lot of potential. \n",
      "\n",
      "What are you most interested in about GenAI? Are you working on any projects yourself? I'd love to hear more about your passions!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# âœ… Pass session_id inside config['configurable']\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": \"chat1\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# âœ… Pass a list of HumanMessage as input\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, my name is Niruppa and I am a GenAI enthusiast.\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1aac9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Niruppa.  \n",
      "\n",
      "I remember that from our introduction! ðŸ˜Š  Is there anything else I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What is my name?\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62551808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I have no memory of past conversations and don't know your name. If you'd like to tell me, I'd be happy to know!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config1 = {\n",
    "    \"configurable\": {\n",
    "        \"session_id\": \"chat2\"\n",
    "    }\n",
    "}\n",
    "response = with_message_history.invoke([HumanMessage(content=\"What's my name?\")], config=config1)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea29b1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi John! \n",
      "\n",
      "It's nice to meet you too. \n",
      "\n",
      "How can I help you today? ðŸ™‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hey My name is john\")],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90babc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since I don't have access to past conversations, I don't know your name.  \n",
      "\n",
      "Would you like to tell me? ðŸ˜Š  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ask again in the same session\n",
    "response = with_message_history.invoke([HumanMessage(content=\"What's my name?\")], config=config1)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44f64be",
   "metadata": {},
   "source": [
    "Using Prompt Templates --\n",
    "Prompt templates help turn raw user information into a format that a language model can work with. If the raw user input is just a message, we can use prompt templates to structure it before passing it to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ab8c0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant.Answer all the question to the best of your ability\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain=prompt|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cc831e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Niruppa! It's nice to meet you. ðŸ˜Š \\n\\nI'm ready to answer your questions. What can I help you with today?  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 31, 'total_tokens': 68, 'completion_time': 0.067272727, 'prompt_time': 0.0014781, 'queue_time': 0.25422582, 'total_time': 0.068750827}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--226cb014-6390-4f5c-82c4-d04af270c63a-0', usage_metadata={'input_tokens': 31, 'output_tokens': 37, 'total_tokens': 68})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is niruppa\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77fda709",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e93bf778",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi My name is Niruppa\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a17637d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Niruppa!  It's nice to meet you too.  \\n\\nWhat can I do for you today? ðŸ˜„  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 78, 'total_tokens': 107, 'completion_time': 0.052727273, 'prompt_time': 0.00235867, 'queue_time': 0.25682536, 'total_time': 0.055085943}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--71afb01a-cbcb-45f6-8196-5c8d3d97b0b8-0', usage_metadata={'input_tokens': 78, 'output_tokens': 29, 'total_tokens': 107})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b2def2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more complex\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant.Answer all questions to the best of your ability in {language}.\"\n",
    "\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a1d809b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hallo Niruppa, schÃ¶n dich kennenzulernen!  Wie kann ich dir heute helfen? ðŸ˜Š\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Niruppa\")],\"language\":\"German\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d5814a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history,input_messages_key=\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76e5baea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hallo Niruppa, schÃ¶n dich kennenzulernen! ðŸ‘‹ \\n\\nWie kann ich dir heute helfen?  \\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat4\"}}\n",
    "response=with_message_history.invoke({\n",
    "    'messages':[HumanMessage(content=\"Hi My name is Niruppa\")],\"language\":\"German\"},\n",
    "    config=config\n",
    ")\n",
    "    \n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b6d464e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dein Name ist Niruppa. ðŸ˜Š \\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke({\n",
    "    'messages':[HumanMessage(content=\"Whats my name\")],\"language\":\"German\"},\n",
    "    config=config\n",
    ")\n",
    "    \n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63493c05",
   "metadata": {},
   "source": [
    "# Managing the Chat Conversation History Using Langchain\n",
    "Proper management of conversation history is crucial, as an unmanaged list of messages can grow unbounded and potentially overflow the context window of the language model (LM). Therefore, it is important to add a step that limits the size of the messages being passed.\n",
    "\n",
    "Understanding trim_messages--\n",
    "The trim_messages helper is used to reduce the number of messages sent to the model. It allows specifying how many tokens to keep and provides other parameters such as whether to always keep the system message and whether to allow partial messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fd78ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.messages import trim_messages\n",
    "\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=70, # limit of tokens\n",
    "    strategy=\"last\", # from the last conversation count the token\n",
    "    token_counter=model, # function for counting tokens\n",
    "    include_system=True, # system message include\n",
    "    allow_partial=False, # no partial information\n",
    "    start_on=\"human\" # start from human conversation\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a good assistant.\"),\n",
    "    HumanMessage(content=\"Hey, I am Bob.\"),\n",
    "    AIMessage(content=\"Hi.\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream.\"),\n",
    "    AIMessage(content=\"Nice.\"),\n",
    "    HumanMessage(content=\"What is two plus two?\"),\n",
    "    AIMessage(content=\"Four.\"),\n",
    "    HumanMessage(content=\"Thanks.\")\n",
    "]\n",
    "\n",
    "trimmed = trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10ae3d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a good assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hey, I am Bob.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Nice.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is two plus two?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Four.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2af60859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You said you like vanilla ice cream. ðŸ˜Š  \\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain =(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    |prompt |model\n",
    ")\n",
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What ice cream do i like \")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f925a058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked \"What is two plus two?\" \\n\\n\\n\\nLet me know if you have any other questions!\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke(\n",
    "    {\n",
    "    \"messages\":messages + [HumanMessage(content=\"What math problem did i ask \")],\n",
    "    \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46beded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap in message history\n",
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history,input_messages_key=\"messages\")\n",
    "config = {\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02f27532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich weiÃŸ nicht, wie du heiÃŸt. KÃ¶nntest du mir deinen Namen sagen? \\n\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=with_message_history.invoke({\n",
    "    'messages':[HumanMessage(content=\"Whats my name\")],\"language\":\"German\"},\n",
    "    config=config\n",
    ")\n",
    "    \n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
